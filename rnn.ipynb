{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) from Scratch\n",
    "Following the [DeepLearning.AI course on Sequence models](https://www.coursera.org/learn/nlp-sequence-models).\n",
    "\n",
    "At each time-step, the RNN tries to predict the next character given the previous ones.\n",
    "\n",
    "- $X = x_1, ... x_{t-1}$\n",
    "- $Y = x_2, ... x_t$\n",
    "\n",
    "where $x_i$ is an input character from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names 4000\n",
      "pets 404\n",
      "dinos 1536\n",
      "vocab_size 61\n",
      "['\\n', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√°', '√©', '√≠', '√±', '√≥', '√∫', '≈ç']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training data\n",
    "names = open(\"datasets/names_ES.txt\", encoding=\"utf-8\").read().split(\"\\n\")[:4000]\n",
    "pets = open(\"datasets/pets_ES.txt\", encoding=\"utf-8\").read().split(\"\\n\")\n",
    "dinos = open(\"datasets/dinos.txt\").read().split(\"\\n\")\n",
    "\n",
    "print(\"names\", len(names))\n",
    "print(\"pets\", len(pets))\n",
    "print(\"dinos\", len(dinos))\n",
    "\n",
    "data = \"\\n\".join(names) + \"\\n\" + \"\\n\".join(pets) + \"\\n\" + \"\\n\".join(dinos)\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model only accepts numeric input, so we could transform each character into a different number, this is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53, '√°': 54, '√©': 55, '√≠': 56, '√±': 57, '√≥': 58, '√∫': 59, '≈ç': 60}\n"
     ]
    }
   ],
   "source": [
    "char2idx = {ch:i for i, ch in enumerate(chars)}\n",
    "idx2char = {i:ch for i, ch in enumerate(chars)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must clip the gradients to prevent it from [exploding](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) üò≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    for gradient in gradients.values():\n",
    "        np.clip(gradient, -maxValue, maxValue, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the next character at time-step $t+1$ we could sample with randomness (to avoid getting always the same character) from the probability distribution obtained through softmax at $Y_t$, where each chacater has its own probability of being the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\te_x = np.exp(x - np.max(x))\n",
    "\treturn e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sample(parameters, char_to_ix):\n",
    "\tWaa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "\n",
    "\tx = np.zeros((vocab_size, 1)) # input chracter one-hot\n",
    "\ta_prev = np.zeros((Waa.shape[1], 1)) # previous hidden state\n",
    "\n",
    "\tindices = []\n",
    "\tidx = -1\n",
    "\n",
    "\tcounter = 0 # stop at 50 characters\n",
    "\tnewline_character = char_to_ix['\\n']\n",
    "\n",
    "\twhile (idx != newline_character and counter != 50):\n",
    "\t\t# Forward propagate x\n",
    "\t\ta = np.tanh(Wax @ x + Waa @ a_prev + ba)\n",
    "\t\ty = softmax(Wya @ a + by)\n",
    "\n",
    "\t\t# Sample the index of a character from the probability distribution y\n",
    "\t\tidx = np.random.choice(vocab_size, p = y.ravel())\n",
    "\t\tindices.append(idx)\n",
    "\n",
    "\t\t# Update x and a_prev for the next iteration\n",
    "\t\tx = np.zeros((vocab_size, 1))\n",
    "\t\tx[idx] = 1\n",
    "\t\ta_prev = a\n",
    "\n",
    "\t\tcounter += 1\n",
    "\tif (counter == 50):\n",
    "\t\tindices.append(char_to_ix['\\n'])\n",
    "\n",
    "\treturn indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(xt, a_prev, parameters):\n",
    "\tWax, Waa, Wya, ba, by = parameters['Wax'], parameters['Waa'], parameters['Wya'], parameters['ba'], parameters['by']\n",
    "\n",
    "\ta = np.tanh(Waa @ a_prev + Wax @ xt + ba) \n",
    "\ty = softmax(Wya @ a + by)\n",
    "\n",
    "\treturn a, y\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters):\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        x[t] = np.zeros((vocab_size, 1)) # one-hot encode the input\n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "            \n",
    "        a[t], y_hat[t] = rnn_step_forward(x[t], a[t - 1], parameters)\n",
    "        \n",
    "        loss -= np.log(y_hat[t][Y[t], 0])\n",
    "\n",
    "    cache = (y_hat, a, x)\n",
    "\n",
    "    return loss, cache\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    gradients['dWya'] += dy @ a.T\n",
    "    gradients['dby'] += dy\n",
    "    Wya = parameters['Wya']\n",
    "    Waa = parameters['Waa']\n",
    "    da = Wya.T @ dy + gradients['da_next'] \n",
    "    dtanh = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['dba'] += dtanh\n",
    "    gradients['dWax'] += dtanh @ x.T\n",
    "    gradients['dWaa'] += dtanh @ a_prev.T\n",
    "    gradients['da_next'] = Waa.T @ dtanh\n",
    "    return gradients\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "\tgradients = {}\n",
    "\n",
    "\t(y_hat, a, x) = cache\n",
    "\tWaa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "\n",
    "\tgradients['dWax'] = np.zeros_like(Wax)\n",
    "\tgradients['dWaa'] = np.zeros_like(Waa)\n",
    "\tgradients['dWya'] = np.zeros_like(Wya)\n",
    "\tgradients['dba'] = np.zeros_like(ba)\n",
    "\tgradients['dby'] = np.zeros_like(by)\n",
    "\tgradients['da_next'] = np.zeros_like(a[0])\n",
    "\n",
    "\t# Backpropagate through time (!)\n",
    "\tfor t in reversed(range(len(X))):\n",
    "\t\tdy = np.copy(y_hat[t])\n",
    "\t\tdy[Y[t]] -= 1 \n",
    "\t\tgradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t - 1])\n",
    "\n",
    "\treturn gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "\tparameters['Wax'] += -lr * gradients['dWax']\n",
    "\tparameters['Waa'] += -lr * gradients['dWaa']\n",
    "\tparameters['Wya'] += -lr * gradients['dWya']\n",
    "\tparameters['ba']  += -lr * gradients['dba']\n",
    "\tparameters['by']  += -lr * gradients['dby']\n",
    "\treturn parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let be optimized, shalle we use [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) as our holy optimization algorithm üòîüôè."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate=0.01):\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    clip(gradients, 5)\n",
    "    \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    Wax = np.random.randn(n_a, n_x) * 0.01\n",
    "    Waa = np.random.randn(n_a, n_a) * 0.01\n",
    "    Wya = np.random.randn(n_y, n_a) * 0.01\n",
    "    ba = np.zeros((n_a, 1))\n",
    "    by = np.zeros((n_y, 1))\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "    return parameters\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "\treturn -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "\treturn loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "def get_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:] # capitalize the first character\n",
    "    return txt\n",
    "\n",
    "def model(data_x, ix_to_char, char_to_ix, num_iterations=35000, n_a=50, names_to_sample=10, verbose=False):\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    loss = get_initial_loss(vocab_size, names_to_sample)\n",
    "    \n",
    "    examples = [x.strip() for x in data_x]\n",
    "    np.random.shuffle(examples)\n",
    "\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    last_name = \"abc\"\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        idx = j % len(examples)\n",
    "        \n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = [char for char in single_example]\n",
    "        single_example_ix = [char_to_ix[char] for char in single_example]\n",
    "        X = [None] + single_example_ix\n",
    "        \n",
    "        ix_newline = char_to_ix['\\n']\n",
    "        Y = X[1:] + [ix_newline]\n",
    "\n",
    "        curr_loss, _, a_prev = optimize(X, Y, a_prev, parameters, learning_rate=0.01)\n",
    "        \n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            for _ in range(names_to_sample):\n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                last_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_name.replace('\\n', ''))\n",
    "                \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters, last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example = Coelurosauravus\n",
      "single_example_chars ['C', 'o', 'e', 'l', 'u', 'r', 'o', 's', 'a', 'u', 'r', 'a', 'v', 'u', 's']\n",
      "single_example_ix [4, 42, 32, 39, 48, 45, 42, 46, 28, 48, 45, 28, 49, 48, 46]\n",
      " X =  [None, 4, 42, 32, 39, 48, 45, 42, 46, 28, 48, 45, 28, 49, 48, 46] \n",
      " Y =        [4, 42, 32, 39, 48, 45, 42, 46, 28, 48, 45, 28, 49, 48, 46, 0] \n",
      "\n",
      "Iteration: 0, Loss: 41.133403\n",
      "\n",
      "Hph√°q√±CjgH√≥fO√≠O√°nf DGSO√≥D≈çnmQ√©YcK≈çezrQe√∫kk√°vLL YNE\n",
      "QTsb√°vRhGXFHvgEQXeFJshorqHVBqWtJbteAk√≠lQQEOTmWXJ√±O\n",
      "WtUbJyeP√∫SBhP√°r√≥ QrZXASksRvqoH√≠qHgDMYZzX√°yctkMWaup\n",
      "OYswQxXsGatJkUFjHbF TSXOB√±pG√±GBxFRZTufoynQokQ√∫SVqX\n",
      "√ÅgqMzwMYZF\n",
      "HsJojgIErdbYmVmbmWOPXiyr√©lDjwgnFHAVOEJW voHAYgWKz√±\n",
      "AssdnKBEwlSe\n",
      "TiSdWaL√±VbN√©mB≈çVWkRrPf√©wWSK√©vFLf√±ya√©GHgI√©hKcP√±v√≠Zy\n",
      "Ci≈çbotGYWbvgZLYNDPVXRdyF√±xwgErCgTNyIBtdIVXst√°MlsKo\n",
      "√çRjTgw√≥sTwoCYUCKK Qwz√≥a√±A≈çz√∫cGohPufvt√°LXa pZwFBltd\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2000, Loss: 43.814698\n",
      "\n",
      "Ariaerusa\n",
      "Ario\n",
      "Mara Ma EuoZLuna √©telPJbieto Jvanola Ariiol Momla\n",
      "Hnaretebdin\n",
      "To\n",
      "Axa raNmi\n",
      "Ai√≥len\n",
      "Zarena\n",
      "Ebihen\n",
      "Alo MeZurasan Olabgo Noxlapu  rna Jaa MarinanodMas\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 38.550184\n",
      "\n",
      "Sona\n",
      "Ratena Mostarra Ciana Gerestaniostonieleracamanaic\n",
      "Selit\n",
      "Pirico Mantetien Jola Alicio Minitna Reel\n",
      "Alhel\n",
      "Ante DelesUNareu Dejxeo\n",
      "IAnga roVa SDlol IREAlYfOalanros\n",
      "Sostenoluetorar\n",
      "Noergen\n",
      "Repo Mhenarodn\n",
      "\n",
      "\n",
      "j =  5939 idx =  5939\n",
      "j =  5940 idx =  0\n",
      "Iteration: 6000, Loss: 36.284504\n",
      "\n",
      "Emico Phuhil\n",
      "Oncal\n",
      "Sandelo Antrisa ElicakKilU\n",
      "EOlizodonie\n",
      "A\n",
      "Sinlis Ks\n",
      "Miria Malin\n",
      "Hilel Minel\n",
      "Elisio Bialll\n",
      "Alvondi\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 35.126840\n",
      "\n",
      "Garillina\n",
      "Nongafar Ckin\n",
      "Anne Der\n",
      "Delasaurus\n",
      "Agsant\n",
      "Dagelsaira DepJEdm\n",
      "Singy Catyrdia\n",
      "Iile Cadela Frndo Gianotia\n",
      "Bradopsaurus\n",
      "Allcipelus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 34.369573\n",
      "\n",
      "JisoloD Feva\n",
      "AjERPELDESABHBYADERCARIITNEQDEAMIERYENIJALARRODSMO\n",
      "Neru\n",
      "Jocilan Merig\n",
      "Delesta Dhikayxescusaurus\n",
      "Pewasta\n",
      "Mandonuano Depanmo\n",
      "Dezizabores\n",
      "Nacila Hatre\n",
      "Yantima SovENailagaus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 34.268525\n",
      "\n",
      "Payriro\n",
      "Roceosous\n",
      "Cora Vela Juriesauros\n",
      "Orolosaurus\n",
      "Iliefe\n",
      "Yamio\n",
      "Adto Eela Jximy Fesel Porlina Hahia Uupaina Luingo\n",
      "Soba\n",
      "Tyulg\n",
      "Ancel Bethiulos\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 33.764396\n",
      "\n",
      "Jamiscaro Milia Cielli\n",
      "Micuer Glir Mereenidi√°Jible Dtia\n",
      "Adihin\n",
      "JredMarie\n",
      "Satildo Dela Dolanchychurus\n",
      "Amilmmila\n",
      "Geonaro Hhilis\n",
      "Etinia\n",
      "Alfiaur\n",
      "Nania\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 33.569837\n",
      "\n",
      "Atalieria\n",
      "Armeras\n",
      "JuLUL Arteacorda Plbarchentops\n",
      "Vallio\n",
      "Tamuarthurot\n",
      "Fhucaptosaurus\n",
      "AmOJWOSTGINEL Ricto\n",
      "ONINYLAliom IARCCNEYHJOORTAR TMELJo Anmey Vartaade\n",
      "Mattio Nuallio\n",
      "Sidoruithauret\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 33.468047\n",
      "\n",
      "Le Yuthestor\n",
      "Ernzsaurus\n",
      "Ana Anablia\n",
      "IIETLETLorodongeler\n",
      "Orgolyo\n",
      "Ingus\n",
      "Widlio\n",
      "Lidie\n",
      "Armin Silvan\n",
      "Ricail\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 33.151635\n",
      "\n",
      "Jeocrisaurus\n",
      "Andel Ffula\n",
      "Varcario Lucels Incheitona Labunoor\n",
      "Scira\n",
      "Marlonyrasupe\n",
      "Wisados\n",
      "Anicia SzIke Oria Antovida Anaviponus Nallon\n",
      "Hichyrtonatucurus\n",
      "Olley Aruelro Benogeops\n",
      "Paucondones Rotana Natlecaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 33.135863\n",
      "\n",
      "Denco Almisaurus\n",
      "Joantaro ChaitONAz\n",
      "Esibola\n",
      "Alejo Juan\n",
      "Efil\n",
      "Riar Delsa\n",
      "Astadaren Marestina Hingertisanertio Justel\n",
      "Macterto Arasaeleplyptyrion\n",
      "Roberdo\n",
      "Ranfy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), idx2char, char2idx, 22001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
