{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) from Scratch\n",
    "Following the [DeepLearning.AI course on Sequence models](https://www.coursera.org/learn/nlp-sequence-models).\n",
    "\n",
    "At each time-step, the RNN tries to predict the next character given the previous ones.\n",
    "\n",
    "- $X = x_1, ... x_t$\n",
    "- $Y = x_2, ... x_{t+1}$\n",
    "\n",
    "where $x_i$ is an input character from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 27\n",
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = open('dinos.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char2idx = {ch:i for i, ch in enumerate(chars)}\n",
    "idx2char = {i:ch for i, ch in enumerate(chars)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must clip the gradients to prevent the [Exploding Gradient problem ‚ò†Ô∏è](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['dba'], gradients['dby']\n",
    "\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -maxValue, maxValue, gradient)\n",
    "\n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"dba\": db, \"dby\": dby}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the next character at time-step $t+1$ we could sample with randomness (to avoid always getting the same character) from the probability distribution obtained through softmax at $Y_t$. The domain of this distributios is the vocabulary, as each character has its own probability of being the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\te_x = np.exp(x - np.max(x))\n",
    "\treturn e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "\tWaa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "\tvocab_size = by.shape[0]\n",
    "\tn_a = Waa.shape[1]\n",
    "\n",
    "\tx = np.zeros((vocab_size, 1)) # one-hot vector for the first character\n",
    "\ta_prev = np.zeros((n_a, 1)) # previous hidden state\n",
    "\n",
    "\tindices = []\n",
    "\tidx = -1\n",
    "\n",
    "\tcounter = 0 # stop at 50 characters\n",
    "\tnewline_character = char_to_ix['\\n']\n",
    "\n",
    "\twhile (idx != newline_character and counter != 50):\n",
    "\t\t# Forward propagate x\n",
    "\t\ta = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)\n",
    "\t\tz = np.dot(Wya, a) + by\n",
    "\t\ty = softmax(z)\n",
    "\n",
    "\t\t# Sample the index of a character within the vocabulary from the probability distribution y\n",
    "\t\tidx = np.random.choice(vocab_size, p = y.ravel())\n",
    "\t\tindices.append(idx)\n",
    "\n",
    "\t\t# Overwrite the input character as the one corresponding to the sampled index.\n",
    "\t\tx = np.zeros((vocab_size, 1))\n",
    "\t\tx[idx] = 1\n",
    "\n",
    "\t\t# Update the hidden state\n",
    "\t\ta_prev = a\n",
    "\n",
    "\t\tcounter += 1\n",
    "\tif (counter == 50):\n",
    "\t\tindices.append(char_to_ix['\\n'])\n",
    "\n",
    "\treturn indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(xt, a_prev, parameters):\n",
    "\tWax = parameters[\"Wax\"]\n",
    "\tWaa = parameters[\"Waa\"] \n",
    "\tWya = parameters[\"Wya\"] \n",
    "\tba = parameters[\"ba\"]  \n",
    "\tby = parameters[\"by\"]   \n",
    "\n",
    "\ta_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba) #(n_a, 1)\n",
    "\tyt_pred = softmax(np.dot(Wya, a_next) + by) #(n_y,1)\n",
    "\n",
    "\treturn a_next, yt_pred\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters, vocab_size=27):\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector.\n",
    "        x[t] = np.zeros((vocab_size, 1))\n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(x[t], a[t - 1], parameters) #a[t]: (n_a,1), y_hat[t]:(n_y,1)\n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t], 0])\n",
    "\n",
    "    cache = (y_hat, a, x)\n",
    "\n",
    "    return loss, cache\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    Wya = parameters['Wya']\n",
    "    Waa = parameters['Waa']\n",
    "    da = np.dot(Wya.T, dy) + gradients['da_next'] \n",
    "    dtanh = (1 - a * a) * da  # backprop through tanh nonlinearity\n",
    "    gradients['dba'] += dtanh\n",
    "    gradients['dWax'] += np.dot(dtanh, x.T)\n",
    "    gradients['dWaa'] += np.dot(dtanh, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(Waa.T, dtanh)\n",
    "\n",
    "    return gradients\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "\t# Initialize gradients as an empty dictionary\n",
    "\tgradients = {}\n",
    "\n",
    "\t# Retrieve from cache and parameters\n",
    "\t(y_hat, a, x) = cache\n",
    "\tWaa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "\n",
    "\t# each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "\tgradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "\tgradients['dba'], gradients['dby'] = np.zeros_like(ba), np.zeros_like(by)\n",
    "\tgradients['da_next'] = np.zeros_like(a[0])\n",
    "\n",
    "\t# Backpropagate through time\n",
    "\tfor t in reversed(range(len(X))):\n",
    "\t\tdy = np.copy(y_hat[t])\n",
    "\t\tdy[Y[t]] -= 1 \n",
    "\t\tgradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t - 1])\n",
    "\n",
    "\treturn gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "\tparameters['Wax'] += -lr * gradients['dWax']\n",
    "\tparameters['Waa'] += -lr * gradients['dWaa']\n",
    "\tparameters['Wya'] += -lr * gradients['dWya']\n",
    "\tparameters['ba']  += -lr * gradients['dba']\n",
    "\tparameters['by']  += -lr * gradients['dby']\n",
    "\treturn parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let be optimized, with [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) as our optimization algorithm üòîüôè."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    gradients = clip(gradients, 5)\n",
    "    \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "\tnp.random.seed(1)\n",
    "\tWax = np.random.randn(n_a, n_x) * 0.01  # input to hidden\n",
    "\tWaa = np.random.randn(n_a, n_a) * 0.01  # hidden to hidden\n",
    "\tWya = np.random.randn(n_y, n_a) * 0.01  # hidden to output\n",
    "\tba = np.zeros((n_a, 1))  # hidden bias\n",
    "\tby = np.zeros((n_y, 1))  # output bias\n",
    "\tparameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "\treturn parameters\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "\treturn -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "\treturn loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "def get_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    return txt\n",
    "\n",
    "def model(data_x, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27, verbose = False):\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    examples = [x.strip() for x in data_x]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    last_dino_name = \"abc\"\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        idx = j % len(examples)\n",
    "        \n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = [char for char in single_example]\n",
    "        single_example_ix = [char_to_ix[char] for char in single_example]\n",
    "        X = [None] + single_example_ix\n",
    "        \n",
    "        ix_newline = char_to_ix['\\n']\n",
    "        Y = X[1:] + [ix_newline]\n",
    "\n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate=0.01)\n",
    "        \n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            #  The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                last_dino_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_dino_name.replace('\\n', ''))\n",
    "                \n",
    "                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters, last_dino_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example = turiasaurus\n",
      "single_example_chars ['t', 'u', 'r', 'i', 'a', 's', 'a', 'u', 'r', 'u', 's']\n",
      "single_example_ix [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19]\n",
      " X =  [None, 20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19] \n",
      " Y =        [20, 21, 18, 9, 1, 19, 1, 21, 18, 21, 19, 0] \n",
      "\n",
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  1535 idx =  1535\n",
      "j =  1536 idx =  0\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382338\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.265759\n",
      "\n",
      "Meustoloplohus\n",
      "Imeda\n",
      "Iutosaurus\n",
      "Maca\n",
      "Yuspanenphurus\n",
      "Daaisicachtitan\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.901372\n",
      "\n",
      "Phytrohaesaurus\n",
      "Melaaisaurus\n",
      "Mystoosaurus\n",
      "Peeamosaurus\n",
      "Ytronosaurus\n",
      "Eiakosaurus\n",
      "Trogonosaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.924847\n",
      "\n",
      "Nixusaurus\n",
      "Llecalosaurus\n",
      "Lxusodon\n",
      "Necalosaurus\n",
      "Ystrengosaurus\n",
      "Eg\n",
      "Trochkosaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.759659\n",
      "\n",
      "Piustolonosaurus\n",
      "Migbaeron\n",
      "Myrrocepholus\n",
      "Peeadosaurus\n",
      "Yusodomincteros\n",
      "Eiadosaurus\n",
      "Trocephods\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), idx2char, char2idx, 22001, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
