{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) from Scratch\n",
    "Following the [DeepLearning.AI course on Sequence models](https://www.coursera.org/learn/nlp-sequence-models).\n",
    "\n",
    "At each time-step, the RNN tries to predict the next character given the previous ones.\n",
    "\n",
    "- $X = x_1, ... x_{t-1}$\n",
    "- $Y = x_2, ... x_t$\n",
    "\n",
    "where $x_i$ is an input character from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names 3000\n",
      "dinos 1536\n",
      "vocab_size 54\n",
      "['\\n', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Training data\n",
    "names = open(\"data/names_3M_ARG.txt\", encoding=\"utf-8\").read().split(\"\\n\")[:3000]\n",
    "dinos = open(\"data/dinos.txt\").read().split(\"\\n\")\n",
    "data = \"\\n\".join(names) + \"\\n\" + \"\\n\".join(dinos)\n",
    "\n",
    "print(\"names\", len(names))\n",
    "print(\"dinos\", len(dinos))\n",
    "\n",
    "vocab = sorted(list(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"vocab_size\", vocab_size)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model only accepts numeric input, so we could transform each character into a different number, this is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8, 'H': 9, 'I': 10, 'J': 11, 'K': 12, 'L': 13, 'M': 14, 'N': 15, 'O': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'U': 22, 'V': 23, 'W': 24, 'X': 25, 'Y': 26, 'Z': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53}\n"
     ]
    }
   ],
   "source": [
    "char2idx = {ch:i for i, ch in enumerate(vocab)}\n",
    "idx2char = {i:ch for i, ch in enumerate(vocab)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must clip the gradients to prevent it from [exploding](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/) üò≥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    for gradient in gradients.values():\n",
    "        np.clip(gradient, -maxValue, maxValue, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the next character at time-step $t+1$ we could sample with randomness (to avoid getting always the same character) from the probability distribution obtained through softmax at $Y_t$, where each chacater has its own probability of being the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\te_x = np.exp(x - np.max(x))\n",
    "\treturn e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sample(parameters, char_to_ix):\n",
    "\tWaa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "\n",
    "\tx = np.zeros((vocab_size, 1)) # input chracter one-hot\n",
    "\ta_prev = np.zeros((Waa.shape[1], 1)) # previous hidden state\n",
    "\n",
    "\tindices = []\n",
    "\tidx = -1\n",
    "\n",
    "\tcounter = 0 # stop at 50 characters\n",
    "\tnewline_character = char_to_ix['\\n']\n",
    "\n",
    "\twhile (idx != newline_character and counter != 50):\n",
    "\t\t# Forward propagate x\n",
    "\t\ta = np.tanh(Wax @ x + Waa @ a_prev + ba)\n",
    "\t\ty = softmax(Wya @ a + by)\n",
    "\n",
    "\t\t# Sample the index of a character from the probability distribution y\n",
    "\t\tidx = np.random.choice(vocab_size, p = y.ravel())\n",
    "\t\tindices.append(idx)\n",
    "\n",
    "\t\t# Update x and a_prev for the next iteration\n",
    "\t\tx = np.zeros((vocab_size, 1))\n",
    "\t\tx[idx] = 1\n",
    "\t\ta_prev = a\n",
    "\n",
    "\t\tcounter += 1\n",
    "\tif (counter == 50):\n",
    "\t\tindices.append(char_to_ix['\\n'])\n",
    "\n",
    "\treturn indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(xt, a_prev, parameters):\n",
    "\tWax, Waa, Wya, ba, by = parameters['Wax'], parameters['Waa'], parameters['Wya'], parameters['ba'], parameters['by']\n",
    "\n",
    "\ta = np.tanh(Waa @ a_prev + Wax @ xt + ba) \n",
    "\ty = softmax(Wya @ a + by)\n",
    "\n",
    "\treturn a, y\n",
    "\n",
    "def rnn_forward(X, Y, a0, parameters):\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        x[t] = np.zeros((vocab_size, 1)) # one-hot encode the input\n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "            \n",
    "        a[t], y_hat[t] = rnn_step_forward(x[t], a[t - 1], parameters)\n",
    "        \n",
    "        loss -= np.log(y_hat[t][Y[t], 0])\n",
    "\n",
    "    cache = (y_hat, a, x)\n",
    "\n",
    "    return loss, cache\n",
    "\n",
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    gradients['dWya'] += dy @ a.T\n",
    "    gradients['dby'] += dy\n",
    "    Wya = parameters['Wya']\n",
    "    Waa = parameters['Waa']\n",
    "    da = Wya.T @ dy + gradients['da_next'] \n",
    "    dtanh = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['dba'] += dtanh\n",
    "    gradients['dWax'] += dtanh @ x.T\n",
    "    gradients['dWaa'] += dtanh @ a_prev.T\n",
    "    gradients['da_next'] = Waa.T @ dtanh\n",
    "    return gradients\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "\tgradients = {}\n",
    "\n",
    "\t(y_hat, a, x) = cache\n",
    "\tWaa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "\n",
    "\tgradients['dWax'] = np.zeros_like(Wax)\n",
    "\tgradients['dWaa'] = np.zeros_like(Waa)\n",
    "\tgradients['dWya'] = np.zeros_like(Wya)\n",
    "\tgradients['dba'] = np.zeros_like(ba)\n",
    "\tgradients['dby'] = np.zeros_like(by)\n",
    "\tgradients['da_next'] = np.zeros_like(a[0])\n",
    "\n",
    "\t# Backpropagate through time (!)\n",
    "\tfor t in reversed(range(len(X))):\n",
    "\t\tdy = np.copy(y_hat[t])\n",
    "\t\tdy[Y[t]] -= 1 \n",
    "\t\tgradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t - 1])\n",
    "\n",
    "\treturn gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, lr):\n",
    "\tparameters['Wax'] += -lr * gradients['dWax']\n",
    "\tparameters['Waa'] += -lr * gradients['dWaa']\n",
    "\tparameters['Wya'] += -lr * gradients['dWya']\n",
    "\tparameters['ba']  += -lr * gradients['dba']\n",
    "\tparameters['by']  += -lr * gradients['dby']\n",
    "\treturn parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let be optimized, shalle we use [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) as our holy optimization algorithm üòîüôè."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate=0.01):\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    clip(gradients, 5)\n",
    "    \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    Wax = np.random.randn(n_a, n_x) * 0.01\n",
    "    Waa = np.random.randn(n_a, n_a) * 0.01\n",
    "    Wya = np.random.randn(n_y, n_a) * 0.01\n",
    "    ba = np.zeros((n_a, 1))\n",
    "    by = np.zeros((n_y, 1))\n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "    return parameters\n",
    "\n",
    "def get_initial_loss(vocab_size, seq_length):\n",
    "\treturn -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "def smooth(loss, cur_loss):\n",
    "\treturn loss * 0.999 + cur_loss * 0.001\n",
    "\n",
    "def get_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:] # capitalize the first character\n",
    "    return txt\n",
    "\n",
    "def model(data_x, ix_to_char, char_to_ix, num_iterations=35000, n_a=50, names_to_sample=10, verbose=False):\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    loss = get_initial_loss(vocab_size, names_to_sample)\n",
    "    \n",
    "    examples = [x.strip() for x in data_x]\n",
    "    np.random.shuffle(examples)\n",
    "\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    last_name = \"abc\"\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        idx = j % len(examples)\n",
    "        \n",
    "        single_example = examples[idx]\n",
    "        single_example_chars = [char for char in single_example]\n",
    "        single_example_ix = [char_to_ix[char] for char in single_example]\n",
    "        X = [None] + single_example_ix\n",
    "        \n",
    "        ix_newline = char_to_ix['\\n']\n",
    "        Y = X[1:] + [ix_newline]\n",
    "\n",
    "        curr_loss, _, a_prev = optimize(X, Y, a_prev, parameters, learning_rate=0.01)\n",
    "        \n",
    "        if verbose and j in [0, len(examples) -1, len(examples)]:\n",
    "            print(\"j = \" , j, \"idx = \", idx,) \n",
    "        if verbose and j in [0]:\n",
    "            print(\"single_example =\", single_example)\n",
    "            print(\"single_example_chars\", single_example_chars)\n",
    "            print(\"single_example_ix\", single_example_ix)\n",
    "            print(\" X = \", X, \"\\n\", \"Y =       \", Y, \"\\n\")\n",
    "        \n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            for _ in range(names_to_sample):\n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                last_name = get_sample(sampled_indices, ix_to_char)\n",
    "                print(last_name.replace('\\n', ''))\n",
    "                \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters, last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j =  0 idx =  0\n",
      "single_example = Inti Santiago\n",
      "single_example_chars ['I', 'n', 't', 'i', ' ', 'S', 'a', 'n', 't', 'i', 'a', 'g', 'o']\n",
      "single_example_ix [10, 41, 47, 36, 1, 20, 28, 41, 47, 36, 28, 34, 42]\n",
      " X =  [None, 10, 41, 47, 36, 1, 20, 28, 41, 47, 36, 28, 34, 42] \n",
      " Y =        [10, 41, 47, 36, 1, 20, 28, 41, 47, 36, 28, 34, 42, 0] \n",
      "\n",
      "Iteration: 0, Loss: 39.905805\n",
      "\n",
      "JyxKDGjliQHdYlVQNJMDQ ImPikdx lxsHPHekPbjUsnBfKcxt\n",
      "TNKaEb\n",
      "PJylQDZJarfgbwwZOWIgZDIXtciQcWhAYNPxHOZsUmsOTRqAOi\n",
      "CVbIVZuMsg l ztOLSRSPfGrpXsL\n",
      "FWcuygnCGlVWOeIfHBPKtAHbYA\n",
      "ZhRXMQhtHrEJ FvWCxtmwJrjspQnMPDwNaTxQosVSmvmKtSdqI\n",
      "OeuHUq EzNyHRmhdarKVGeLQX CjVsj\n",
      "FDozaiKVsHMWkSwDSkKhGBEwboPeYxmlAbXwLMULEscsqNbCvT\n",
      "TOWLSprOXtAtJidbsDQbpJpTInCZBRecTVKWxrF waboPYyiGF\n",
      "JnN wH\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 43.699245\n",
      "\n",
      "Mamso FicMo Cel\n",
      "Genaa Tcano\n",
      "A\n",
      "Aldona IEsza Bliro Longolinina Ga\n",
      "EnuidP Ranmis Dosa\n",
      "Daures\n",
      "Susoao\n",
      "Evcol\n",
      "Nan laOEuroa AnavocIo\n",
      "Jano Alna ityra Egusiaro\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 39.079623\n",
      "\n",
      "El Melezgo Alelagas\n",
      "KAd ZonDanFarosmurus\n",
      "Y\n",
      "Felan\n",
      "Tidelionurus\n",
      "Aver Eleen\n",
      "EQelado\n",
      "Bdulia ADSeenyGdronl\n",
      "LeelCanhOBlxLar\n",
      "Eliccesaurus\n",
      "\n",
      "\n",
      "j =  4535 idx =  4535\n",
      "j =  4536 idx =  0\n",
      "Iteration: 6000, Loss: 36.643019\n",
      "\n",
      "Sbire Ilivena\n",
      "Evia Meresauros\n",
      "Enirgar\n",
      "Udranodosaurup\n",
      "Bla  piko\n",
      "SibinadMMNIHma Juzotosagora\n",
      "Mobinusaurus\n",
      "Sand Dathete\n",
      "EbabibasEurir Angpsaurus\n",
      "Endes\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 35.393652\n",
      "\n",
      "Lasalepho Adesthosaurus\n",
      "Nael Crareria Honystonitrgs\n",
      "Lais MaelecotaGJue\n",
      "Luenushus\n",
      "Wioles Maraselen Ebvaphonisaurus\n",
      "Ilice\n",
      "OndilenaPEOEbie Noses\n",
      "Aasanelo EsLOlosaurus\n",
      "Vaneotera\n",
      "Kmaberminoscus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 34.702228\n",
      "\n",
      "Tra der Adeleotor\n",
      "LiT\n",
      "AZeestrsaurus\n",
      "Jodanax\n",
      "Tirlo de Gonnian\n",
      "Vangel Bennio Miliano OHNAROR\n",
      "Lian\n",
      "Chla Camina Madra  Orxona Balceltos\n",
      "Jeicouca Calia  Izerlosaurus\n",
      "Hishorossus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 34.144546\n",
      "\n",
      "Yicerosauras\n",
      "Martar\n",
      "AlJan\n",
      "AmilincatoravMilla Nian\n",
      "Urasurops\n",
      "Elarelo Jus\n",
      "Ixacedorus\n",
      "Ayelodesufuratureci\n",
      "Hamar Estacia Gelll\n",
      "Lais Sitia\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 34.116023\n",
      "\n",
      "Janguistom\n",
      "Iharilta Oelesgdia\n",
      "Jankila\n",
      "Roelles\n",
      "Panthaysaurus\n",
      "Ievhael\n",
      "Lergy Latxulistury\n",
      "EYFarmimialdo SamonolJorila Corenia\n",
      "Jorlminiana Lenelestilus\n",
      "Hedranieli\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 33.945726\n",
      "\n",
      "Raberasadrasaurus\n",
      "Dimala ClyCTeselda\n",
      "ILLENAXVEMIBEJYVONUVia Mabasterancadin\n",
      "Raela Pell Luantenur\n",
      "Solerma\n",
      "Luvusela Laylden\n",
      "Salentos Andrita Lemen Ale\n",
      "Edeferdo Iselenda\n",
      "Egelliana\n",
      "Simantina Enber\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 34.013153\n",
      "\n",
      "Majamengod\n",
      "Viasa Gisiano FasSALA\n",
      "Jaia Milia\n",
      "Elmel\n",
      "Rasento Geura Alge Viresia\n",
      "GazAMLEN\n",
      "Raplona\n",
      "Luvado Denze Caria Maruel Ponstia\n",
      "Picarelia\n",
      "Camil Adulopel\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 33.300562\n",
      "\n",
      "Marco Sanidol\n",
      "Terilo Gabrig\n",
      "Jamon\n",
      "Taren Gabel\n",
      "Nardos Maur\n",
      "Falled Dana Elila Iele\n",
      "Jaforo Emario\n",
      "Dimo Jean Canis\n",
      "Eztar Aldonseonyerur\n",
      "Sranna Daniel\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 33.714421\n",
      "\n",
      "Canira Mauria Agbel\n",
      "Marcita Delosaurus\n",
      "Lista Veraill\n",
      "Dertyu Abertantia\n",
      "Ing scita Parcaldonto\n",
      "Roosaria\n",
      "Mal\n",
      "Liomangesus\n",
      "Jorosayrus\n",
      "Fothphyna Marto\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters, last_name = model(data.split(\"\\n\"), idx2char, char2idx, 22001, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
